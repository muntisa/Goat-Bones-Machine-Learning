{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MAs for a set of experimental conditions - Bones project\n",
    "\n",
    "The model will try to classify bone parameters 'Fw', 'Fl', 'Fd', 'Hw', 'Hl', 'Hd' using 6 metabolism results 'PTH', 'BALP', 'BGLAP', 'INTP', 'TRAP', 'CTX-I' for 3 types of experimental conditions 'Animal', 'Treat', 'Period':\n",
    "- PTH\tParathyroid hormone\tng/ml\n",
    "- BALP\tBone Alkaline Phosphatase\tmU/ml\n",
    "- BGLAP\tBone Gla protein\tng/ml\n",
    "- INTP\tCross-linked N-terminal Telopeptides of type I collagen\tng/ml\n",
    "- TRAP\tTartrate Resistant Acid Phosphatase\tU/L\n",
    "- CTX-I\tCross-linked C-terminal Telopeptides of type I collagen\tng/ml\n",
    "- Fw\tFemur_weight\tg\n",
    "- Fl\tFemur_length\tmm\n",
    "- Fd\tFemur_diamater\tmm\n",
    "- Hw\thumerus_weight\tg\n",
    "- Hl\thumerus_length\tmm\n",
    "- Hd\thumerus_diamater\tmm\n",
    "\n",
    "Gestation periods:\n",
    "- middle\tmiddle-gestation\n",
    "- late\tlate-gestation\n",
    "\n",
    "Animal:\n",
    "- Fetus\tFetus\n",
    "- Maternal\tMon\n",
    "\n",
    "Experimental Conditions (EC):\n",
    "- two animal resources (fetus, maternal) for femur and humerus\n",
    "- two different treatments (control group and nutrition restriction group)\n",
    "- two different gestation periods (middle- and late- gestations)\n",
    "\n",
    "**The script will do:**\n",
    "- Data integration - Integrate metabolism and bone data using 'SerNum','Animal','Treat','Period'; this way we are sure that both data are from the same experiment;\n",
    "- Output variables transformation - Z-score transform of the output variables depending on Animal + class transform using a cutoff (the bone parameters depends on the animal type)\n",
    "- MAmix and MAi - feature engineering for inputs using experimental conditions and metabolism; MAmix = MA for a set of EC (EC1,EC2,EC3); MAi = MA using each type of EC.\n",
    "- all these transformation will be saved as intermediate datasets in *datasets*.\n",
    "\n",
    "**Note** - We will try two methodologies:\n",
    "1. Transformed MAs for the metabolism data using ECs (differences of original features with the average value in specific ECs); we will use MA for mixed ECs and MA for each type of EC. In addition, we add probabilities of mixed or individual EC in the dataset. In *dataset2*, we will combine all these input features to create **72 datasets** for each output variables.\n",
    "2. Classical ML with one hot representation of the ECs and original metabolism data (no MAs!); this method uses EC information directly as input binary features for each values of an EC (1/0): 'Animal_Fetus', 'Animal_Mon', 'Treat_Con', 'Treat_Res', 'Period_Late', 'Period_Mid'. We will create **6 datasets** in *datasets3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset integration = bones + metabolism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read bone and metabolism datasets as dataframes\n",
    "df_params = pd.read_csv('./datasets/bone_params.csv')\n",
    "df_metabolites = pd.read_csv('./datasets/bone_metabolites.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bones data\n",
    "df_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metabolism data\n",
    "df_metabolites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check *null* values in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of Null values in df_params = ', df_params.isnull().values.sum())\n",
    "print('No of Null values in df_metabolites = ', df_metabolites.isnull().values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate datasets** with bone dimensions and metabolism concentrations using serum Id and experimental conditions to be sure that the SerNum corresponds to the same experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_params, df_metabolites, on=['SerNum','Animal','Treat','Period'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save integrated dataset\n",
    "print('-> Saving new dataset ...')\n",
    "df.to_csv('./datasets/ds.bones_raw.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataset dimension\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new datasset columns\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process categorical nominal experimental conditions\n",
    "\n",
    "This part is dedicated to the classical ML methodolody where the ECs are directly included as one-hot representations in the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of categorical experimental conditions\n",
    "df_cat = df[['Animal', 'Treat', 'Period']].copy()\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of values for each categorical feature\n",
    "df_cat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of each EC\n",
    "print(df_cat['Animal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of each EC\n",
    "print(df_cat['Treat'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of each EC\n",
    "print(df_cat['Period'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for one-hot transform\n",
    "df_cat_onehot = df_cat.copy()\n",
    "df_cat_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform original ECs in one-hot features\n",
    "df_cat_onehot = pd.get_dummies(df_cat_onehot, \n",
    "                               columns=['Animal', 'Treat', 'Period'], \n",
    "                               prefix = ['Animal', 'Treat', 'Period'])\n",
    "df_cat_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add these transformed features to the initial datasframe\n",
    "df = pd.concat([df, df_cat_onehot], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new dataset with one-hot ECs as inputs\n",
    "print('-> Saving new dataset ...')\n",
    "df.to_csv('./datasets/ds.bones_raw_onehot.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling of output variable\n",
    "\n",
    "http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "\n",
    "Because we shall search for classification models, we need to transform the 6 output variables into classes (Fw, Fl, Fd, Hw, Hl, Hd). First, let's check for outliers in order to decide what type of nornalization to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outs = df[['SerNum','Animal','Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd']].copy()\n",
    "df_outs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using direct observation on output variables and metabolism data, we observe that there is a clear separation between the ranges for Fetus and Mon. This means that if we choose to transform in classes using the average values of the outputs, the resulted classes will be 1 for Mom and 0 for Fetus because always the smaller values are from Fetus and the largest values are from Mon. We will see the differences in the following boxplots. Therefore, this transformation has no sense because we need to classify if an output has low or high value for both types of animals. In conclusion, we need to transform in classes using separated groups of animals.\n",
    "\n",
    "Separate data by Aminal type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outs_Fetus = df_outs[df_outs['Animal']=='Fetus'].copy()\n",
    "df_outs_Fetus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outs_Fetus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot output variables for Fetus to see the range value and the possible outliers\n",
    "df_outs_Fetus.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for Mon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outs_Mon = df_outs[df_outs['Animal']=='Mon'].copy()\n",
    "df_outs_Mon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outs_Mon.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot output variables for Mon to see the range value and the possible outliers\n",
    "df_outs_Mon.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion about outliers**: we observ outliers in all the output variables for both type of animals. Thus, the safest method is to use **Robust scaller**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasnform the output variables for Fetus\n",
    "\n",
    "scaler = RobustScaler()\n",
    "robust_scaled_df = scaler.fit_transform(df_outs_Fetus[['Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd']].copy())\n",
    "\n",
    "# save the scaler \n",
    "joblib.dump(scaler, \"scaler_Fetus_outs.save\")\n",
    "\n",
    "# create a dataframe with the scaled data\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd'])\n",
    "robust_scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate scaled data to df\n",
    "df_outs_Fetus_scaled = robust_scaled_df.copy()\n",
    "df_outs_Fetus_scaled['SerNum']= list(df_outs_Fetus['SerNum'])\n",
    "df_outs_Fetus_scaled['Animal']= list(df_outs_Fetus['Animal'])\n",
    "df_outs_Fetus_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data\n",
    "df_outs_Fetus_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasnform the output variables for Mon\n",
    "\n",
    "scaler2 = RobustScaler()\n",
    "robust_scaled_df2 = scaler2.fit_transform(df_outs_Mon[['Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd']].copy())\n",
    "\n",
    "# save the scaler\n",
    "joblib.dump(scaler2, \"scaler_Mon_outs.save\")\n",
    "\n",
    "# create a dataframe with the scaled data\n",
    "robust_scaled_df2 = pd.DataFrame(robust_scaled_df2, columns=['Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd'])\n",
    "robust_scaled_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate scaled data to df\n",
    "df_outs_Mon_scaled = robust_scaled_df2.copy()\n",
    "df_outs_Mon_scaled['SerNum']= list(df_outs_Mon['SerNum'])\n",
    "df_outs_Mon_scaled['Animal']= list(df_outs_Mon['Animal'])\n",
    "df_outs_Mon_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outs_Mon_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled to Class\n",
    "\n",
    "Because we used Robust scaler, the cutoff will be 0. Thus, the class transformed outputs will have value of 1 if the output is greater than 0.0 (median of output values) and 0 if the output value will be less than 0.0 (median of output values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.0 # due to Robust scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform of output scaled values to classes for Fetus\n",
    "\n",
    "df_outs_Fetus_class = df_outs_Fetus_scaled.copy()\n",
    "for output in ['Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd']:\n",
    "    df_outs_Fetus_class[output].values[df_outs_Fetus_class[output] >= 0] = 1\n",
    "    df_outs_Fetus_class[output].values[df_outs_Fetus_class[output] <  0] = 0\n",
    "df_outs_Fetus_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform of output scaled values to classes for Mon\n",
    "\n",
    "df_outs_Mon_class = df_outs_Mon_scaled.copy()\n",
    "for output in ['Fw', 'Fl', 'Fd', 'Hw','Hl', 'Hd']:\n",
    "    df_outs_Mon_class[output].values[df_outs_Mon_class[output] >= 0] = 1\n",
    "    df_outs_Mon_class[output].values[df_outs_Mon_class[output] <  0] = 0\n",
    "df_outs_Mon_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append both animal type output variables transformed in classes\n",
    "df_outs_class = df_outs_Fetus_class.append(df_outs_Mon_class)\n",
    "print(df_outs_class.shape)\n",
    "print(df_outs_class.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df with df_outs_class\n",
    "\n",
    "print(df.columns)\n",
    "print(df_outs_class.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.merge(df.drop(['Fw', 'Fl', 'Fd', 'Hw', 'Hl', 'Hd'],axis = 1), \n",
    "                    df_outs_class, on=['SerNum','Animal'])\n",
    "df_class.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new dataset\n",
    "print('-> Saving new dataset ...')\n",
    "df_class.to_csv('./datasets/ds.bones_class_allOuts.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment-centered scaling (Moving Averages) \n",
    "\n",
    "On other methodology is to calculate the MAs of the original metabolism input features using ECs. This way, the input features will be differences with respect with the average values in specific ECs. Thus, we will not use one-hot encoding of ECs but we will encode the EC information into the generated MAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sets of columns\n",
    "experim  = ['Animal', 'Treat', 'Period']\n",
    "features = ['PTH', 'BALP', 'BGLAP', 'INTP', 'TRAP', 'CTX-I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform original features into MAs using ECs\n",
    "\n",
    "print('-> Creating MAs ...')\n",
    "df_raw = df_class.copy()\n",
    "for descr in features:  # for each input feature of metabolism\n",
    "    # calculate the Avg for a pair of experimental conditions and a descriptor\n",
    "    avgs = df_class.groupby(experim, as_index = False).agg({descr:\"mean\"})\n",
    "    \n",
    "    # rename the avg column name\n",
    "    avgs = avgs.rename(columns={descr: 'avg-'+ descr + '-' + 'experim'})\n",
    "    \n",
    "    # merge an Avg to dataset\n",
    "    df_raw = pd.merge(df_raw, avgs, on=experim)\n",
    "    \n",
    "    # add MA to the dataset for pair Exp cond - descr\n",
    "    df_raw['MA-'+descr+'-'+'experim'] = df_raw[descr] - df_raw['avg-'+descr+'-'+'experim']\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new columns\n",
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> Saving the dataset with all MA details ...')\n",
    "df_raw.to_csv('./datasets/ds.bones_MA_details.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
